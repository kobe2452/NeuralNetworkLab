{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "# polarity dataset v2.0\n",
    "# 1000 positive and 1000 negative movie reviews\n",
    "TEXT_DATA_DIR = 'review_polarity/txt_sentoken/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_texts_labels(TEXT_DATA_DIR):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    # iterate through directories to read each file\n",
    "    for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "        if not name.startswith('.'):\n",
    "            path = os.path.join(TEXT_DATA_DIR, name)\n",
    "            if os.path.isdir(path):\n",
    "                for fname in sorted(os.listdir(path)):\n",
    "                    fpath = os.path.join(path, fname)\n",
    "                    with open(fpath, 'r') as myfile:\n",
    "                        doc = myfile.read().replace('\\n', '')\n",
    "                        # store each review and label respectively\n",
    "                        texts.append(doc)\n",
    "                        if name == 'pos':\n",
    "                            labels.append(+1)\n",
    "                        else:\n",
    "                            labels.append(0)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2000\n"
     ]
    }
   ],
   "source": [
    "# Check the numbers of texts and labels\n",
    "texts, labels = prepare_texts_labels(TEXT_DATA_DIR)\n",
    "print(len(texts), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3860.642\n",
      "14898\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "# Check the average/maximum/minimum length of reviews in characters\n",
    "total_avg = sum( map(len, texts) ) / len(texts)\n",
    "print(total_avg)\n",
    "print(len(max(texts, key=len)))\n",
    "print(len(min(texts, key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW in SpaCy V2: https://spacy.io/models/en#en_vectors_web_lg\n",
    "nlp = spacy.load('en_vectors_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "778.0345 2810 18\n"
     ]
    }
   ],
   "source": [
    "# Tokenize reviews into a doc object\n",
    "# Check the average/maximum/minimum length of reviews in tokens \n",
    "length_list = []\n",
    "\n",
    "for i, review in enumerate(texts):\n",
    "    doc = nlp.make_doc(review)\n",
    "    length_list.append(len(doc))\n",
    "\n",
    "print(np.mean(length_list), max(length_list), min(length_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800 1800 900 900\n",
      "200 200 100 100\n"
     ]
    }
   ],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split data (and labels) into training and testing subsets\n",
    "# Stratify the labels for both subsets (i.e., equal numbers of pos/neg reviews)\n",
    "# Avoid imbalanced class distributions\n",
    "X_train_data, X_test_data, y_train_label, y_test_label = train_test_split(texts, labels, test_size=0.1, random_state=5, stratify=labels)\n",
    "print(len(X_train_data), len(y_train_label), y_train_label.count(+1), y_train_label.count(0))\n",
    "print(len(X_test_data), len(y_test_label), y_test_label.count(+1), y_test_label.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only up to N first words in each review\n",
    "# This is ad-hoc for this exercise. Other decisions could be made.\n",
    "MAX_SEQUENCE_LENGTH = 777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions customized based on https://spacy.io/docs/usage/deep-learning\n",
    "# Using a simple concatenate representation in this exercise, not treating embedding as a whole \n",
    "def get_features(docs, MAX_SEQUENCE_LENGTH, nlp):\n",
    "    Xs = np.empty((len(list(docs)), MAX_SEQUENCE_LENGTH*nlp.vocab.vectors_length), dtype=np.float64)\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc = nlp.make_doc(doc)\n",
    "        for j, token in enumerate(doc[:MAX_SEQUENCE_LENGTH]):\n",
    "            if token.has_vector:\n",
    "                vector = token.vector\n",
    "            else:\n",
    "                vector = np.zeros(nlp.vocab.vectors_length)\n",
    "            for k in range(0, nlp.vocab.vectors_length):\n",
    "                Xs[i, j*nlp.vocab.vectors_length + k] = vector[k]\n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 233100) float64\n"
     ]
    }
   ],
   "source": [
    "# Get features for training data\n",
    "X_train = get_features(X_train_data, MAX_SEQUENCE_LENGTH, nlp)\n",
    "print(X_train.shape, X_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 233100) float64\n"
     ]
    }
   ],
   "source": [
    "# Get features for testing data\n",
    "X_test = get_features(X_test_data, MAX_SEQUENCE_LENGTH, nlp)\n",
    "print(X_test.shape, X_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800,) float64\n",
      "(200,) float64\n"
     ]
    }
   ],
   "source": [
    "# Converts class labels to numpy arrays\n",
    "y_train = np.asarray(y_train_label, dtype='float64')\n",
    "print(y_train.shape, y_train.dtype)\n",
    "y_test = np.asarray(y_test_label, dtype='float64')\n",
    "print(y_test.shape, y_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Sequential model with a linear stack of layers\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a hidden layer\n",
    "# operation in Dense layer: output = activation(dot(input, weight) + bias)\n",
    "model.add(Dense(units=1024, input_dim=MAX_SEQUENCE_LENGTH*nlp.vocab.vectors_length, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an output layer\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the learning process before model training\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1800/1800 [==============================] - 98s 54ms/step - loss: 7.3451 - acc: 0.5078\n",
      "Epoch 2/5\n",
      "1800/1800 [==============================] - 65s 36ms/step - loss: 7.6878 - acc: 0.5039\n",
      "Epoch 3/5\n",
      "1800/1800 [==============================] - 65s 36ms/step - loss: 5.2951 - acc: 0.5878\n",
      "Epoch 4/5\n",
      "1800/1800 [==============================] - 68s 38ms/step - loss: 3.1926 - acc: 0.6783\n",
      "Epoch 5/5\n",
      "1800/1800 [==============================] - 68s 38ms/step - loss: 1.3754 - acc: 0.8156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10e656908>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model, iterating on the data in batches of 128 samples, 5 epochs\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 1024)              238695424 \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 239,220,737\n",
      "Trainable params: 239,220,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print a summary of your model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 2s 11ms/step\n",
      "[1.7375077867507935, 0.68499999284744262]\n",
      "acc: 68.50%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model against the 10% validation data in terms of the loss value & metrics values\n",
    "loss_and_metrics = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print(loss_and_metrics)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], loss_and_metrics[1]*100))\n",
    "# Results may vary, because data are split randomly in cell [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_confusion_matrix(X_true, y_true):\n",
    "\n",
    "    probabilities = model.predict(X_true)\n",
    "    \n",
    "    y_pred = []\n",
    "    for prob in probabilities:\n",
    "        if prob[0] > 0.5:\n",
    "            y_pred.append(+1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "\n",
    "    print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[60 40]\n",
      " [23 77]]\n"
     ]
    }
   ],
   "source": [
    "get_confusion_matrix(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Stratified 10-Folds cross-validator\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
    "# Avoid constructing folds with imbalanced class distributions\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=False, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the above process for 10 times to get average performance metrics\n",
    "cvscores = []\n",
    "\n",
    "X = np.asarray(texts)\n",
    "Y = np.asarray(labels)\n",
    "\n",
    "for train, test in kfold.split(X, Y):\n",
    "    \n",
    "    X_train = get_features(X[train].tolist(), MAX_SEQUENCE_LENGTH, nlp)    \n",
    "    y_train = np.asarray(Y[train], dtype='float64')\n",
    "\n",
    "    X_test = get_features(X[test].tolist(), MAX_SEQUENCE_LENGTH, nlp)    \n",
    "    y_test = np.asarray(Y[test], dtype='float64')\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=128, input_dim=MAX_SEQUENCE_LENGTH*nlp.vocab.vectors_length, activation=\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=128, verbose=0)\n",
    "    \n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    \n",
    "    get_confusion_matrix(X_test, y_test)\n",
    "    \n",
    "    cvscores.append(scores[1] * 100)\n",
    "\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete session from keras backend to free sources\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
